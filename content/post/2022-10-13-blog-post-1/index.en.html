---
title: Blog Post 1
author: Team RRRRR
date: '2022-10-13'
slug: []
categories: []
tags: []
description: ~
toc: yes
authors: []
series: []
lastmod: '2022-10-13T11:28:01-04:00'
featuredVideo: ~
featuredImage: ~
---


<div id="TOC">

</div>

<div id="data-set-1" class="section level2">
<h2>Data Set 1</h2>
<p>The link to the original data source is: <a href="https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis" class="uri">https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis</a></p>
<p>This dataset is about customers’ purchase information at a company. In total, the dataset has 2240 rows and 29 columns. It contains variables that describe the characteristics of customers, such as marital status, kids number in the family, etc. It also has information about the amount spent on some particular products such as wine and fruit. We think that the data was originally collected because the company wants to analyze which kind of products sell the best so that they can make effective decisions regarding inventories and campaigns to targeted customer groups. And, the data is collected from the company’s own sales records.</p>
<p>Since the dataset is not very large and we have a pretty clear topic to explore, we think we are able to clean the data.Regarding the question that we want to explore, we are going to examine the relationship between the product sale and characteristics of customers. For example, which characteristics of customers, such as education or kids number at home, have the largest impact on the sale of a particular product, such as wine. We are also interested in the relationship between the in-store purchase and some particular kind of products. For example, if more in-store purchases are correlated with more fish and fruit purchases. Lastly, we also want to examine whether income influences customers’ number of deal purchases. The challenge that we might encounter is that the dataset is a little bit small that we may not be able to find a clear pattern.</p>
</div>
<div id="data-set-2" class="section level2">
<h2>Data Set 2</h2>
<p>The link to the original data source is: <a href="https://catalog.data.gov/dataset/highway-rail-grade-crossing-accident-data" class="uri">https://catalog.data.gov/dataset/highway-rail-grade-crossing-accident-data</a></p>
<p>This dataset is created by collecting the surveys of accidents of HIGHWAY-RAIL GRADE CROSSING ACCIDENT/INCIDENT REPORT. This is a extremely big database (241519 rows and 159 columns) which contains the information of how did the highway-rail crossing accidents happened in different railroad, which kinds of highway mobile were involved in these accidents, and the situation of the incidents during when it happened(including weather condition, highway and railroad condition, and visibility condition). All the information gathered in this dataset comes from the report.</p>
<p>Though the dataset is big(&gt;200 mb), we are able to load it and clean it, by filtering out the railroad that has the highest number of incidents. Our group is aiming to discover the main reasons that lead to these accidents, whether accidents are caused mostly by natural factors or human factors, by looking at temperature, weather, visibility, vs. circumstances and speed. Analyzing the circumstances of hazardous materials involved, we’re able to answer its relationship with the severity of the accident, whether this would result in the driver to be killed, and the property damage in dollar terms. By obtaining the main causes of the high accident rate, we could reach some recommendations on the actions people can take to prevent them.</p>
<p>The first challenge we foresee is to decide which columns we want to keep as we are cleaning the data. To do analysis with these huge column numbers(159 columns), we have to carefully select the variables which influence the accident situation. Another challenge is that as some of the data is out-dated, the description(Narrative) column of these out-dated data states none, which gives us no clue about what exactly happened during the accident and that we have to do the analysis based on our own understanding of the situation and the data we get cleaned out.</p>
</div>
<div id="data-set-3" class="section level2">
<h2>Data Set 3</h2>
<p>The link to the original data source is: <a href="https://catalog.data.gov/dataset/traffic-crashes-resulting-in-injury-victims-involved" class="uri">https://catalog.data.gov/dataset/traffic-crashes-resulting-in-injury-victims-involved</a></p>
<p>This table contains all victims (parties who are injured) involved in a traffic crash resulting in an injury in the City of San Francisco. Traffic crash injury data is collected from the California Highway Patrol 555 Crash Report as submitted by the police officer within 30 days after the crash occurred. All fields that match the SWITRS data schema are programmatically extracted, de-identified, geocoded, and loaded into TransBASE. This dataset originally contain 68343 rows and 88 columns. With each row representing a case that meets the San Francisco Vision Zero Fatality Protocol, columns show the records of Locations, time, involved parties, Weather, Lighting conditions, Severity of accident, details of the victim, finance response, etc. The tables contain information on each crash, from all parties involved in the crashes, and about each party injured in the collision.</p>
<p>The goal of our team is to find out the factors that are more likely to cause traffic accidents by observing coordinates (including longitude, latitude and area code), collision types (e.g. cars and bicycles), number of deaths, number of injuries, road type, lighting, collision time and other data, so as to draw conclusions about which roads in the city need to be repaired. In addition, we help traffic police judge the severity of traffic accidents as soon as they get the information, and also provide reference and travel guide for San Francisco citizens. Most of the data we need is perfect, and we avoid data that is difficult to quantify to ensure that we can clean the data smoothly. One of the predictable challenges is, how do you use data such as latitude and longitude, area codes and street names to delimit areas into a quantified regional data.</p>
</div>
