---
title: Data
description:
toc: true
featuredVideo:
featuredImage: images/data-import-cheatsheet-thumbs.png
draft: false
---

## Section 1: Original Dataset

The main dataset for our project is the [Highway-Rail Grade Crossing accident data](https://catalog.data.gov/dataset/highway-rail-grade-crossing-accident-data). This data was published on Oct. 18, 2022, and was collected by Federal Railroad Administration by collecting the surveys of accidents of HIGHWAY-RAIL GRADE CROSSING ACCIDENT/INCIDENT REPORT. Furthermore, this dataset was collected for the latter researchers to finger out the reasons why some locations have a higher rate of accidents, whether it is because of the visibility or other reasons, such as temperature and the signaling systems, which is exactly what our team is interested in. 
Since this is an extremely big database (241519 rows and 159 columns), we only focus on the data for Massachusetts ranging from 1975 to 2022 for our project. 

The following are the relevant variables for the original datasets:

* `Report Years`, `Date`, `Month`, `Day`, `Hour`, `Minute`, `AM/PM`, `Time`: accurate time elements of when the accidents happened
* `County Name`: the county where the accidents took places
* `Estimated Vehicle Speed`: the vehicle speed in miles per hour when the accidents took place 
* `Temperature`: the temperature (in Fahrenheit) at which the accidents took place
* `Visibility`: this visibility during the time at which the accidents took place. Possible values are `Day`, `Dark`, `Dawn`, `Dusk`
* `Weather Condition`: the weather condition at the places where the acccidents took place. Possible values are `Clear`, `Cloundy`, `Rain`, `Snow`, `Sleet`

## Section 1.1: Cleaning and Loading the Original Dataset

As mentioned, the original dataset has a lot of columns with unnecessary columns, which results in missing information that we wanted to omit. Also, as we were only considering one specific state to study its accident numbers and factors that influence the accident rate, we cleaned the data in our next step. 

So, for our EDA, we began by filtering out the data from Massachusetts, and removed variables we viewed as irrelevant to the possible causes of an accident or the coded columns that are repeated, such as types of crossing warning, number of vehicle occupants, reporting parties, etc. Next, since we found a typo of mistakenly recording “WORCESTER” as “WORCHESTER” during our initial data exploration, we used `str_replace_all` function to solve this problem. Finally, we checked that there were no parsing issues in our original cleaned dataset MA_select, which can be found [here](https://drive.google.com/file/d/1OdrOlaBwO8MlDSL78BxI74YHk4wpAQhN/view). 

## Section 1.2: Potential Limitations of our Dataset

One issue about our main dataset MA_select is that there are a lot of missing values or unknown for some variables. This happens even for those variables that we are interested in. Therefore, we will need to drop those values and this may affect the performance of our analysis. In addition, there are confounding variables that are not in the main dataset, which is the traffic volume variable. Luckily, we found another dataset that records the traffic volume of counties in Massachusetts. However, even the traffic volume dataset has its own limitations. It does not have data regarding some counties, such as Suffolk, so for the traffic volume, we only focus on those counties that have data on traffic volume. However, when we only look at the years from 2017 to 2022, there are only around 60 pieces of data. As a result, on average, there is only 1 piece of data per month. Therefore, there may not be enough data in the main datasets for us to perform regression analysis. To still see the relationship between the traffic volume and the accidents count, we plan to draw series plots to see if there are similar trends between them. 


## Section 2: Merging Datasets and Cleaning Merged Datasets 

In addition to exploratory data analysis of the original dataset, we wanted to see if traffic volume has any impact on the count of accidents in Massachusettes. In order to do so, we further combine the main dataset with another dataset, the [MA traffic volume](https://mhd.public.ms2soft.com/tcds/tsearch.asp?loc=Mhd&mod=) by county. 

This supplementary data was collected by the Highway Division of the Massachusetts Department of Transportation in order to analyze and predict the traffic flow, which provides instruction on things like extending some roads. And we manually picked the data from the website, calculated the average traffic volume by month, and summarized it in the MA traffic volume dataset. Since the earlier years do not have much meaning in representing the near future situation, we only use the traffic volume data ranging from 2017 to 2022. The TrafficVolume dataset can be found [here](https://docs.google.com/spreadsheets/d/1mIT5NyYcTCREjJJ238Hj4ZIP0e5PvEb5-W9vbEptRgs/edit#gid=0)

The following are the variables in the MA traffic volume dataset: 

* `County`: counties from Massachusetts that correspond to MA_select to assure the same research subject

* `Year`, `Month`: the specific year and month at the date when the traffic volume was collected

* `Count`: the average number of cars that count as the traffic volume during the month 

## Section 3: R Packages

We only use one additional R packages when cleaning our dataset.

* stringr: making it easy to work with strings – we use it to fix the typo problem 

## Section 4: Code for Cleaning the Datasets

Our load_and_clean_data.R details of the original data MA_select can be found in detail below. We used Excel to manually combine the data points from the website for the MA traffic volume dataset, and saved the csv file here. 

```{r}
library(tidyverse)
library(readr)

# read the original dataset
HighwayRail<-read_csv(here::here("dataset-ignore", "Highway-Rail_Grade_Crossing_Accident_Data.csv"), show_col_types = FALSE)

# clean the data, only keep data in MA
HighwayRail_MA <- HighwayRail %>% filter(`State Name` == "MASSACHUSETTS") 

# select necessary column names for MA in the original dataset
MA_select <- HighwayRail_MA %>%
  select(`Report Year`, Date,	Month, Day, Hour, Minute, `AM/PM`, Time, `Nearest Station`, 	Subdivision,	`County Name`, `State Name`,`Highway Name`,`Public/Private Code`, `Public/Private`, `Highway User`, `Estimated Vehicle Speed`,`Vehicle Direction`,	`Highway User Position`,`Equipment Involved`, `Equipment Struck`, Temperature, Visibility, `Weather Condition`, `Number of Locomotive Units`, `Number of Cars`, `Train Speed`, `Estimated/Recorded Speed`,	 `Train Direction`, `Crossing Warning Expanded 1`,`Crossing Warning Expanded 2`,`Crossing Warning Expanded 3`,`Crossing Warning Expanded 4`,`Crossing Warning Expanded 5`,`Crossing Warning Expanded 6`,`Crossing Warning Expanded 7`,`Crossing Warning Expanded 8`,`Crossing Warning Expanded 9`, `Crossing Warning Expanded 10`, `Crossing Warning Expanded 11`,	`Crossing Warning Expanded 12`,	`Signaled Crossing Warning Code`, `Signaled Crossing Warning`, `Roadway Condition`,	`Crossing Warning Location`,	`Warning Connected To Signal`, `Crossing Illuminated`,`User Age`,`User Gender`,`User Struck By Second Train`, `Highway User Action`, `View Obstruction`, `Driver Condition`, `Driver In Vehicle`, `Crossing Users Killed For Reporting Railroad`, `Crossing Users Injured For Reporting Railroad`, `Vehicle Damage Cost`,	`Employees Killed For Reporting Railroad`, `Employees Injured For Reporting Railroad`, `Passengers Killed For Reporting Railroad`, `Passengers Injured For Reporting Railroad`, Narrative)

# fixing the typo 
library('stringr')
MA_select$`County Name` <- str_replace_all(MA_select$`County Name`, "WORCHESTER", "WORCESTER")

write_csv(MA_select, file = here::here("dataset", "MA_select.csv"))

save(MA_select, file = here::here("dataset/MA_select.RData"))

# read the combined dataset
TrafficVolume <- read_csv(here::here("dataset", "Traffic Volume.csv"), show_col_types = FALSE)
save(TrafficVolume, file = here::here("dataset/MA_select.RData"))
```

----

[Delete before the final version ]

## Rubric: On this page

you will

* Describe where/how to find data.
  * You must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.
  * Why was the data collected/curated? Who put it together? (This is important, if you don't know why it was collected then that might not be a good dataset to look at.
* Describe the different data files used and what each variable means. 
  * If you have many variables then only describe the most relevant ones and summarize the rest.
* Describe any cleaning you had to do for your data.
  * You *must* include a link to your `load_and_clean_data.R` file.
  * Also, describe any additional R packages you used outside of those covered in class.
  * Describe and show code for how you combined multiple data files and any cleaning that was necessary for that.
  * Some repetition of what you do in your `load_and_clean_data.R` file is fine and encouraged if it helps explain what you did.
* Organization, clarity, cleanliness of the page
  * Make sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.